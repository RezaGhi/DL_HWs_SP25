{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yKp1NLIIia1"
      },
      "source": [
        "<img src='sharif_logo.png' alt=\"SUT logo\" width=150 height=150 align=left class=\"saturate\" >\n",
        "\n",
        "<br>\n",
        "<font face=\"Times New Roman\">\n",
        "<div dir=ltr align=center>\n",
        "<font color=0F5298 size=7>\n",
        " Deep Learning <br>\n",
        "<font color=2565AE size=5>\n",
        "Computer Engineering Department - Spring 2025  <br>\n",
        "<font color=3C99D size=5>\n",
        "          Homework 2:  <br>\n",
        "<font color=696880 size=4>\n",
        "           \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment Overview\n",
        "\n",
        "In this assignment, you will explore inference scaling techniques in large language models (LLMs) and evaluate their performance using the Math Benchmark. Throughout the notebook, you will learn about several inference methods, including:\n",
        "\n",
        "- **Chain-of-Thought (CoT):** A method where the model generates intermediate reasoning steps before providing the final answer.\n",
        "- **Best-of-n Sampling:** An approach that generates multiple candidate responses and selects the best one based on a scoring function.\n",
        "- **Beam Search:** A technique that expands several possible sequences simultaneously, choosing the most promising ones based on probability.\n",
        "- **Self-Refinement:** An iterative process where the model revises its output to improve accuracy and coherence.\n",
        "\n",
        "The **Math Benchmark** is a suite of challenging mathematical problems designed to test the reasoning and problem-solving capabilities of LLMs. The benchmark includes a variety of questions ranging from basic arithmetic and algebra to more advanced topics such as geometry and calculus. For example, you might be asked to solve an equation like `2x + 5 = 15` or compute the derivative of a function, tasks that assess the model's ability to handle both straightforward and complex mathematical queries.\n",
        "\n",
        "By the end of this assignment, you will have:\n",
        "- Gained a deeper understanding of inference time scaling methods in LLMs.\n",
        "- Compared the effectiveness of different inference techniques using a rigorous math evaluation framework.\n",
        "\n",
        "Let's dive into the notebook and begin exploring how these methods perform on a challenging set of math problems!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## vLLM: Accelerated Inference Engine for LLMs\n",
        "\n",
        "vLLM is an open-source project designed to optimize the loading and inference of large language models. By leveraging advanced memory management techniques and dynamic batching, vLLM significantly speeds up the inference process, making it easier to deploy and experiment with LLMs even on hardware with limited resources\n",
        "So we use vLLM to get results faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyG3Ng0jIl7m"
      },
      "outputs": [],
      "source": [
        "!pip install vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWvzLP5FIuc4"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY4FWoryOiry"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade numpy\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivC7LIgkLUvR"
      },
      "source": [
        "\n",
        "This command launches a vLLM inference server with:\n",
        "- Model: `DeepSeek-R1-Distill-Qwen-1.5B`\n",
        "- Port: `8000` (default API endpoint)\n",
        "- Precision: `half` (FP16) for memory efficiency\n",
        "- Max context length: `3192` tokens\n",
        "\n",
        "**Note:**  \n",
        "ðŸ”¹ Ensure you're using a GPU runtime (T4 or better) in Colab  \n",
        "ðŸ”¹ Only run the next cell if this one executes successfully \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HNmCEaGIzEe"
      },
      "outputs": [],
      "source": [
        "!vllm serve \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"   --port 8000   --dtype=half   --max-model-len 3192"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFPitveIL8PT"
      },
      "source": [
        "* this cell lunches model in background using vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZSaM7q5LrC0",
        "outputId": "46867f87-5f17-462b-e769-470532c66399"
      },
      "outputs": [],
      "source": [
        "!nohup vllm serve \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\" --port 8000 --dtype=half --max-model-len 5192 &"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0yGjrAiGXWs"
      },
      "source": [
        "## LLM Query Function\n",
        "\n",
        "* This Python function sends prompts to a locally-hosted LLM API and returns the generated response\n",
        "* you can change max_tokens and temperature as you want\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r-QntB7EF4mb"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "def get_llm_response(prompt):\n",
        "    url = \"http://localhost:8000/v1/chat/completions\"\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "\n",
        "        ],\n",
        "    \"max_tokens\": 500,\n",
        "    \"temperature\": 0.6\n",
        "    }\n",
        "    response = requests.post(url, json=payload)\n",
        "    return response.json()['choices'][0]['message']['content'].strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzqLI_OPMNOD"
      },
      "source": [
        "# Test response generation\n",
        "- testing model with some Math benchmark quesions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PG8o90EML-4",
        "outputId": "5215a402-c5bc-4f86-b059-a5c30228dbb9"
      },
      "outputs": [],
      "source": [
        "# TODO: Generate a response with these Math benchmark quesions \n",
        "question1 = \"How many positive whole-number divisors does 196 have?\"\n",
        "# real answer : 9\n",
        "question2 = \"What is the distance, in units, between the points $(2, -6)$ and $(-4, 3)$? Express your answer in simplest radical form.\"\n",
        "# real answer = 3\\\\sqrt{13}\n",
        "question3 = \"Define\\n\\\\[p = \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{k^2} \\\\quad \\\\text{and} \\\\quad q = \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{k^3}.\\\\]Find a way to write\\n\\\\[\\\\sum_{j = 1}^\\\\infty \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\]in terms of $p$ and $q.$\"\n",
        "# real answer = p - q\n",
        "\n",
        "response = \n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Math Benchmark Evaluation\n",
        "\n",
        "This cell is dedicated to evaluating the performance of inference scaling methods on the Math Benchmark dataset. The process works as follows:\n",
        "\n",
        "- **Dataset Loading:** It loads the MATH-500 dataset, which contains a set of challenging math problems along with their correct solutions.\n",
        "- **Answer Extraction:** The `extract_answer` function is used to parse and extract the final answer from the generated responses. This function specifically looks for a LaTeX-style format (using `\\boxed{...}`) to reliably pinpoint the answer.\n",
        "- **Normalization and Comparison:** Before comparing, both the predicted answer and the ground truth are normalized using several functions. These functions handle different mathematical expressions, such as fractions, matrices, and algebraic expressions, ensuring that the comparison is fair and accurate regardless of formatting differences.\n",
        "- **Evaluation Loop:** For each problem:\n",
        "  - The ground truth answer is extracted from the provided solution.\n",
        "  - A response is generated by the LLM using a designated function.\n",
        "  - The predicted answer is then extracted and compared against the ground truth.\n",
        "  - The results for each problem, including whether the predicted answer is correct, are saved for later analysis.\n",
        "- **Results Analysis:** After processing all problems, the cell aggregates the results and prints a summary, including the total number of problems evaluated, the number of correct answers, and the overall accuracy.\n",
        "\n",
        "This evaluation method ensures that the output of each inference technique (such as Chain-of-Thought, Best-of-n, Beam Search, and Self-Refinement) is consistently measured against the Math Benchmark, without altering the original answers or evaluation logic.\n",
        "\n",
        "**Note:**  \n",
        "\n",
        "ðŸ”¹ you don't need to modify this cell. Only rewrite the evaluation function portion then\n",
        "\n",
        "ðŸ”¹ you need to run this cell before evaluating.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jgnyeyBxE2Ci"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "from typing import Dict, Optional, Union\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# Load the MATH-500 dataset\n",
        "def load_math500_dataset():\n",
        "    dataset = load_dataset(\"HuggingFaceH4/MATH-500\")[\"test\"]\n",
        "    return dataset\n",
        "\n",
        "# Extract the last boxed answer from text\n",
        "def extract_answer(response: str) -> Optional[str]:\n",
        "    if not response:\n",
        "        return None\n",
        "    start_idx = response.rfind('\\\\boxed{')\n",
        "    if start_idx == -1:\n",
        "        return None\n",
        "    brace_count = 1\n",
        "    pos = start_idx + 7  # length of '\\boxed{'\n",
        "    while pos < len(response) and brace_count > 0:\n",
        "        if response[pos] == '{':\n",
        "            brace_count += 1\n",
        "        elif response[pos] == '}':\n",
        "            brace_count -= 1\n",
        "        pos += 1\n",
        "    if brace_count == 0:\n",
        "        answer = response[start_idx + 7:pos - 1]\n",
        "        return answer.strip()\n",
        "    return None\n",
        "\n",
        "# Normalization and comparison functions (unchanged from original)\n",
        "def normalize_number(num_str: str) -> str:\n",
        "    try:\n",
        "        cleaned = re.sub(r'[,\\$\\\\]|\\s*(?:cm|m|kg|ft|in|lb|oz|ml|L)$|\\s*\\\\text{[^}]+}', '', num_str).strip()\n",
        "        if cleaned.startswith('.'):\n",
        "            cleaned = '0' + cleaned\n",
        "        num = float(cleaned)\n",
        "        if abs(num) < 1 and '.' in cleaned:\n",
        "            decimal_places = len(cleaned.split('.')[1])\n",
        "            format_str = f\"{{:.{decimal_places}f}}\"\n",
        "            result = format_str.format(num)\n",
        "        else:\n",
        "            result = str(num)\n",
        "        return result\n",
        "    except:\n",
        "        return num_str\n",
        "\n",
        "def numerically_equal(str1: str, str2: str) -> bool:\n",
        "    try:\n",
        "        return abs(float(str1) - float(str2)) < 1e-10\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def normalize_fraction(fraction_str: str) -> str:\n",
        "    try:\n",
        "        fraction_str = fraction_str.replace('\\\\dfrac', '\\\\frac')\n",
        "        fraction_str = ''.join(fraction_str.split())\n",
        "        fraction_str = re.sub(r'\\s*\\\\text{[^}]+}', '', fraction_str)\n",
        "        mixed_brace = re.match(r'^\\\\frac(\\d+)\\{(\\d+)\\}$', fraction_str)\n",
        "        if mixed_brace:\n",
        "            num, den = mixed_brace.groups()\n",
        "            return f\"\\\\frac{{{num}}}{{{den}}}\"\n",
        "        no_braces = re.match(r'^\\\\frac(\\d+)(\\d+)$', fraction_str)\n",
        "        if no_braces:\n",
        "            num, den = no_braces.groups()\n",
        "            return f\"\\\\frac{{{num}}}{{{den}}}\"\n",
        "        if '/' in fraction_str and not any(c in fraction_str for c in '\\\\{}'):\n",
        "            num, den = fraction_str.split('/')\n",
        "            return f\"\\\\frac{{{num.strip()}}}{{{den.strip()}}}\"\n",
        "        standard = re.match(r'^\\\\frac\\{([^{}]+)\\}\\{([^{}]+)\\}$', fraction_str)\n",
        "        if standard:\n",
        "            num, den = standard.groups()\n",
        "            return f\"\\\\frac{{{num}}}{{{den}}}\"\n",
        "    except:\n",
        "        return fraction_str\n",
        "\n",
        "def normalize_matrix_entry(entry: str) -> str:\n",
        "    entry = ''.join(entry.split())\n",
        "    if '/' in entry and not any(c in entry for c in '\\\\{}'):\n",
        "        if entry.startswith('-'):\n",
        "            num, den = entry[1:].split('/')\n",
        "            return f\"-{num.strip()}/{den.strip()}\"\n",
        "        else:\n",
        "            num, den = entry.split('/')\n",
        "            return f\"{num.strip()}/{den.strip()}\"\n",
        "    entry = entry.replace('\\\\dfrac', '\\\\frac')\n",
        "    frac_match = re.match(r'^(-)?\\\\frac\\{(\\d+)\\}\\{(\\d+)\\}$', entry)\n",
        "    if frac_match:\n",
        "        sign, num, den = frac_match.groups()\n",
        "        sign = sign if sign else ''\n",
        "        return f\"{sign}{num}/{den}\"\n",
        "    return entry\n",
        "\n",
        "def normalize_matrix(matrix_str: str) -> str:\n",
        "    try:\n",
        "        matrix_str = ''.join(matrix_str.split())\n",
        "        match = re.match(r'^\\\\begin\\{pmatrix\\}(.*?)\\\\end\\{pmatrix\\}$', matrix_str)\n",
        "        if not match:\n",
        "            return matrix_str\n",
        "        content = match.group(1)\n",
        "        rows = content.split('\\\\\\\\')\n",
        "        normalized_rows = []\n",
        "        for row in rows:\n",
        "            if '&' in row:\n",
        "                entries = [normalize_matrix_entry(entry) for entry in row.split('&')]\n",
        "            else:\n",
        "                entries = [normalize_matrix_entry(row)]\n",
        "            normalized_rows.append('&'.join(entries))\n",
        "        result = \"\\\\begin{pmatrix}\" + \"\\\\\\\\\".join(normalized_rows) + \"\\\\end{pmatrix}\"\n",
        "        return result\n",
        "    except:\n",
        "        return matrix_str\n",
        "\n",
        "def normalize_algebraic_expression(expr: str) -> str:\n",
        "    try:\n",
        "        expr = ''.join(expr.split())\n",
        "        monomial_match = re.match(r'^(-?\\d*\\.?\\d*)?([a-zA-Z])(?:\\^(-?\\d+))?$', expr)\n",
        "        if monomial_match:\n",
        "            coeff, var, exp = monomial_match.groups()\n",
        "            coeff = coeff if coeff and coeff not in ['+', '-'] else ('1' if not coeff else '-1')\n",
        "            exp = exp if exp else '1'\n",
        "            if coeff == '1' and exp == '1':\n",
        "                return var\n",
        "            elif coeff == '1':\n",
        "                return f\"{var}^{exp}\"\n",
        "            elif coeff == '-1' and exp == '1':\n",
        "                return f\"-{var}\"\n",
        "            elif coeff == '-1':\n",
        "                return f\"-{var}^{exp}\"\n",
        "            elif exp == '1':\n",
        "                return f\"{coeff}{var}\"\n",
        "            else:\n",
        "                return f\"{coeff}{var}^{exp}\"\n",
        "        pi_term_match = re.match(r'^(-?\\d*\\.?\\d*)\\\\?pi$', expr)\n",
        "        if pi_term_match:\n",
        "            coeff = pi_term_match.group(1)\n",
        "            if not coeff or coeff == '-':\n",
        "                coeff = '-1' if coeff == '-' else '1'\n",
        "            return f\"{coeff}\\\\pi\"\n",
        "        frac_pi_match = re.match(r'^\\\\frac{([^{}]+)}{([^{}]+)}\\\\?pi$', expr)\n",
        "        if frac_pi_match:\n",
        "            num, den = frac_pi_match.groups()\n",
        "            return f\"\\\\frac{{{num}}}{{{den}}}\\\\pi\"\n",
        "        frac_match = re.match(r'^\\\\frac{([^{}]+)}{([^{}]+)}$', expr)\n",
        "        if frac_match:\n",
        "            num, den = frac_match.groups()\n",
        "            return f\"\\\\frac{{{num}}}{{{den}}}\"\n",
        "    except:\n",
        "        return expr.lower()\n",
        "\n",
        "def normalize_interval_bound(bound: str) -> str:\n",
        "    if '\\\\infty' in bound:\n",
        "        sign = '-' if bound.startswith('-') else ''\n",
        "        return f\"{sign}\\\\infty\"\n",
        "    return normalize_answer(bound) or bound\n",
        "\n",
        "def normalize_interval(interval_str: str) -> str:\n",
        "    try:\n",
        "        interval_str = ''.join(interval_str.split())\n",
        "        match = re.match(r'^\\\\left?([\\[\\(])(.*?),(.*?)\\\\right?([\\]\\)])$', interval_str)\n",
        "        if not match:\n",
        "            match = re.match(r'^([\\[\\(])(.*?),(.*?)([\\]\\)])$', interval_str)\n",
        "            if not match:\n",
        "                return interval_str\n",
        "        left_bracket, left_bound, right_bound, right_bracket = match.groups()\n",
        "        norm_left = normalize_interval_bound(left_bound)\n",
        "        norm_right = normalize_interval_bound(right_bound)\n",
        "        return f\"\\\\left{left_bracket}{norm_left},{norm_right}\\\\right{right_bracket}\"\n",
        "    except:\n",
        "        return interval_str\n",
        "\n",
        "def normalize_ordered_tuple(tuple_str: str) -> str:\n",
        "    try:\n",
        "        tuple_str = tuple_str.replace('\\\\dfrac', '\\\\frac')\n",
        "        tuple_str = tuple_str.replace('\\\\left', '').replace('\\\\right', '')\n",
        "        tuple_str = re.sub(r'\\\\?\\s+', '', tuple_str)\n",
        "        inner = tuple_str.strip('()')\n",
        "        parts = inner.split(',')\n",
        "        normalized_parts = [normalize_answer(part.strip()) for part in parts if normalize_answer(part.strip())]\n",
        "        return f\"({','.join(normalized_parts)})\"\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def normalize_answer(answer: str) -> str:\n",
        "    if answer is None:\n",
        "        return \"\"\n",
        "    answer = re.sub(r'\\\\text{[^}]+(?:inches|feet|meters|cm|m|kg|ft|in|lb|oz|ml|L|per|second|minute|hour)[^}]*}', '', answer)\n",
        "    answer = re.sub(r'(?<!\\\\)\\s+', '', answer)\n",
        "    ordered_pair_match = re.match(r'^(?:\\\\left)?\\((.*?)(?:\\\\right)?\\)$', answer)\n",
        "    if ordered_pair_match:\n",
        "        content = ordered_pair_match.group(1)\n",
        "        parts = content.split(',')\n",
        "        normalized_parts = [normalize_answer(part) for part in parts if normalize_answer(part)]\n",
        "        return f\"({','.join(normalized_parts)})\"\n",
        "    answer = ''.join(answer.split())\n",
        "    if not answer:\n",
        "        return None\n",
        "    pm_match = re.match(r'^(.*?)(?:\\\\pm|-)(.*?)$', answer)\n",
        "    if pm_match:\n",
        "        left, right = pm_match.groups()\n",
        "        norm_left = normalize_answer(left) if left else \"\"\n",
        "        norm_right = normalize_answer(right) if right else \"\"\n",
        "        if norm_left or norm_right:\n",
        "            return f\"{norm_left}\\\\pm{norm_right}\"\n",
        "    trig_match = re.match(r'^\\\\(?:sin|cos|tan|cot|sec|csc)\\s*([a-zA-Z])$', answer)\n",
        "    if trig_match:\n",
        "        variable = trig_match.group(1)\n",
        "        func_name = re.match(r'^\\\\(.*?)(?:\\s|$)', answer).group(1)\n",
        "        return f\"\\\\{func_name}{variable}\"\n",
        "    text_match = re.match(r'^(?:\\\\text{)?([A-Za-z]+)(?:})?$', answer)\n",
        "    if text_match:\n",
        "        return text_match.group(1).lower()\n",
        "    if (answer.startswith('\\\\left[') or answer.startswith('\\\\left(') or\n",
        "        answer.startswith('[') or answer.startswith('(')) and \\\n",
        "       (answer.endswith('\\\\right]') or answer.endswith('\\\\right)') or\n",
        "        answer.endswith(']') or answer.endswith(')')):\n",
        "        return normalize_interval(answer)\n",
        "    if answer.startswith('\\\\begin{pmatrix}') and answer.endswith('\\\\end{pmatrix}'):\n",
        "        return normalize_matrix(answer)\n",
        "    answer = answer.replace('\\\\dfrac', '\\\\frac')\n",
        "    if '\\\\frac' in answer or '/' in answer:\n",
        "        return normalize_fraction(answer)\n",
        "    neg_sqrt_match = re.match(r'^-\\\\sqrt\\{?(\\d+)\\}?$', answer)\n",
        "    if neg_sqrt_match:\n",
        "        num = neg_sqrt_match.group(1)\n",
        "        return f\"-\\\\sqrt{{{num}}}\"\n",
        "    sqrt_match = re.match(r'^(\\d*)?\\\\sqrt\\{?(\\d+)\\}?$', answer)\n",
        "    if sqrt_match:\n",
        "        coeff, num = sqrt_match.groups()\n",
        "        coeff = coeff if coeff else '1'\n",
        "        return f\"\\\\sqrt{{{num}}}\" if coeff == '1' else f\"{coeff}\\\\sqrt{{{num}}}\"\n",
        "    sqrt_with_coeff_match = re.match(r'^(\\d+)\\\\sqrt\\{?(\\d+)\\}?$', answer)\n",
        "    if sqrt_with_coeff_match:\n",
        "        coeff, num = sqrt_with_coeff_match.groups()\n",
        "        return f\"{coeff}\\\\sqrt{{{num}}}\"\n",
        "    base_match = re.match(r'^(\\d+)(?:_\\{?(\\d+)\\}?|_(\\d+))$', answer)\n",
        "    if base_match:\n",
        "        number, base1, base2 = base_match.groups()\n",
        "        base = base1 if base1 else base2\n",
        "        return f\"{number}_{base}\"\n",
        "    percent_match = re.match(r'^(\\d+(?:\\.\\d*)?)\\s*\\\\?%$', answer)\n",
        "    if percent_match:\n",
        "        return normalize_number(percent_match.group(1))\n",
        "    unit_match = re.match(r'^(\\d+(?:\\.\\d*)?)\\s*(?:(?:\\\\[,\\s])|,)?\\s*(?:\\\\\\\\)?(?:\\\\text{(\\w+)}|\\\\?(?:cm|m|kg|ft|in|lb|oz|ml|L))$', answer)\n",
        "    if unit_match:\n",
        "        return normalize_number(unit_match.group(1))\n",
        "    currency_match = re.match(r'^\\\\?\\$?([\\d,]+\\.?\\d*)$', answer)\n",
        "    if currency_match:\n",
        "        return normalize_number(currency_match.group(1))\n",
        "    if re.match(r'^-?[\\d,]+$', answer):\n",
        "        return normalize_number(answer)\n",
        "    unit_match = re.match(r'^(-?[\\d,]+(?:\\.\\d*)?)\\s*(?:\\\\(?:mbox|text|hbox|displaystyle)\\{[^}]+\\})?(?:\\^?\\d)?$', answer)\n",
        "    if unit_match:\n",
        "        return normalize_number(unit_match.group(1))\n",
        "    mc_match = re.match(r'^\\\\text{\\(?([A-Za-z])\\)?}$|^\\(?([A-Za-z])\\)?$', answer)\n",
        "    if mc_match:\n",
        "        return (mc_match.group(1) or mc_match.group(2)).lower()\n",
        "    degree_match = re.match(r'^(-?[\\d,]+(?:\\.\\d*)?)\\s*(?:(?:\\^?\\\\circ)|(?:{\\\\circ})|(?:Â°))?$', answer)\n",
        "    if degree_match:\n",
        "        return normalize_number(degree_match.group(1))\n",
        "    answer = re.sub(r'\\\\text{([^{}]+)}', r'\\1', answer)\n",
        "    try:\n",
        "        return normalize_algebraic_expression(answer)\n",
        "    except:\n",
        "        pass\n",
        "    answer = answer.replace('\\\\left', '').replace('\\\\right', '')\n",
        "    answer = answer.replace('\\\\(', '(').replace('\\\\)', ')')\n",
        "    answer = answer.replace('\\\\[', '[').replace('\\\\]', ']')\n",
        "    answer = answer.replace('\\\\{', '{').replace('\\\\}', '}')\n",
        "    answer = re.sub(r'\\\\sqrt\\{?(\\d+)\\}?', r'\\\\sqrt{\\1}', answer)\n",
        "    answer = re.sub(r'\\\\sqrt{([^{}]+)}', r'\\\\sqrt\\1', answer)\n",
        "    if re.match(r'^\\d+\\\\%$', answer) or re.match(r'^\\d+$', answer):\n",
        "        answer = re.sub(r'\\\\%$', '', answer)\n",
        "    answer = re.sub(r'\\\\text{([^{}]+)}', r'\\1', answer)\n",
        "    while len(answer) >= 2 and answer[0] == '{' and answer[-1] == '}':\n",
        "        if '\\\\frac' in answer:\n",
        "            break\n",
        "        answer = answer[1:-1]\n",
        "    return answer.lower() if answer else None\n",
        "\n",
        "def compare_answers(correct_answer: str, predicted_answer: Optional[str]) -> bool:\n",
        "    if predicted_answer is None:\n",
        "        return False\n",
        "    if numerically_equal(correct_answer, predicted_answer):\n",
        "        return True\n",
        "    normalized_correct = normalize_answer(correct_answer)\n",
        "    normalized_predicted = normalize_answer(predicted_answer)\n",
        "    if not normalized_correct or not normalized_predicted:\n",
        "        return False\n",
        "    if normalized_correct == \"\" and normalized_predicted == \"\":\n",
        "        return False\n",
        "    if ('\\\\left[' in normalized_correct or '\\\\left(' in normalized_correct) and \\\n",
        "       ('\\\\left[' in normalized_predicted or '\\\\left(' in normalized_predicted):\n",
        "        return normalized_correct == normalized_predicted\n",
        "    return normalized_correct == normalized_predicted\n",
        "\n",
        "# Load existing results\n",
        "def load_existing_results(filename: str) -> list[Dict]:\n",
        "    try:\n",
        "        with open(filename, 'r') as f:\n",
        "            return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        return []\n",
        "\n",
        "# Save a single result\n",
        "def save_result(filename: str, result: Dict):\n",
        "    results = load_existing_results(filename)\n",
        "    results.append(result)\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "# Analyze and print results\n",
        "def analyze_results(results: list[Dict]):\n",
        "    total = len(results)\n",
        "    correct = sum(1 for r in results if r['is_correct'])\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    print(\"\\n=== Results Summary ===\")\n",
        "    print(f\"Total problems: {total}\")\n",
        "    print(f\"Correct answers: {correct}\")\n",
        "    print(f\"Accuracy: {accuracy:.2%}\")\n",
        "    print(\"\\n=== Incorrect Problems ===\")\n",
        "    for r in results:\n",
        "        if not r['is_correct']:\n",
        "            print(f\"Problem {r['index']}:\")\n",
        "            print(f\"Expected: {r['correct_answer']}\")\n",
        "            print(f\"Predicted: {r['predicted_answer']}\")\n",
        "            print(\"---\")\n",
        "\n",
        "# Main evaluation function\n",
        "def evaluate():\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    results_file = \"evaluation_results_math500_deepseek.json\"\n",
        "    dataset = load_math500_dataset()\n",
        "    existing_results = load_existing_results(results_file)\n",
        "    processed_indexes = {result['index'] for result in existing_results}\n",
        "    cnt = 0\n",
        "    t=0\n",
        "    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n",
        "        if idx in processed_indexes:\n",
        "            continue\n",
        "        t += 1\n",
        "        problem_text = item['problem']\n",
        "        correct_answer = extract_answer(item['solution'])  # Extract from 'solution', not 'answer'\n",
        "        response = get_llm_response(problem_text)\n",
        "        predicted_answer = extract_answer(response)\n",
        "        is_correct = compare_answers(correct_answer, predicted_answer)\n",
        "        result = {\n",
        "            \"index\": idx,\n",
        "            \"problem\": problem_text,\n",
        "            \"response\": response,\n",
        "            \"correct_answer\": correct_answer,\n",
        "            \"predicted_answer\": predicted_answer,\n",
        "            \"is_correct\": is_correct\n",
        "        }\n",
        "        save_result(results_file, result)\n",
        "        if is_correct:\n",
        "          cnt += 1\n",
        "        print(f\"cnt :  {cnt} idx: {t}\")\n",
        "    final_results = load_existing_results(results_file)\n",
        "    analyze_results(final_results)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNUWF2U_HC8g"
      },
      "source": [
        "# Customizable CoT Prompt Template\n",
        "* modify cot prompt then evaluate on math benchmark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# final answer should be in this format: (because of extract_answer function you can change it if you want)\n",
        "#\\\\[\n",
        "#\\\\boxed{your_answer_here}\n",
        "#\\\\]\n",
        "\n",
        "COT_PROMPT = "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* generate response with cot prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "58w5yeo9GrP2"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def get_COT_response(problem):\n",
        "    prompt = COT_PROMPT + \"\\n\" + problem\n",
        "    url = \"http://localhost:8000/v1/chat/completions\"\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "\n",
        "        ],\n",
        "    \"max_tokens\": 1900,\n",
        "    \"temperature\": 0.3\n",
        "    }\n",
        "    response = requests.post(url, json=payload)\n",
        "    return response.json()['choices'][0]['message']['content'].strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Una6ucDAHb2_"
      },
      "source": [
        "# Evaluate CoT\n",
        "* modify response generation part to evalute this method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Na6kksqZHbOq"
      },
      "outputs": [],
      "source": [
        "def evaluate_cot():\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    results_file = \"evaluation_results_math500_deepseek_cot.json\"\n",
        "    dataset = load_math500_dataset()\n",
        "    existing_results = load_existing_results(results_file)\n",
        "    processed_indexes = {result['index'] for result in existing_results}\n",
        "    cnt = 0\n",
        "    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n",
        "        if idx in processed_indexes:\n",
        "            continue\n",
        "        if idx >= 30:\n",
        "          break\n",
        "        problem_text = item['problem']\n",
        "        correct_answer = extract_answer(item['solution'])\n",
        "        \n",
        "        # TODO: Generate a response with cot\n",
        "        response = \n",
        "        predicted_answer = extract_answer(response)\n",
        "        ##########################################################\n",
        "        is_correct = compare_answers(correct_answer, predicted_answer)\n",
        "        result = {\n",
        "            \"index\": idx,\n",
        "            \"problem\": problem_text,\n",
        "            \"response\": response,\n",
        "            \"correct_answer\": correct_answer,\n",
        "            \"predicted_answer\": predicted_answer,\n",
        "            \"is_correct\": is_correct\n",
        "        }\n",
        "        save_result(results_file, result)\n",
        "        if is_correct:\n",
        "          cnt += 1\n",
        "        print(f\"corrects :  {cnt} idx: {idx}\")\n",
        "    final_results = load_existing_results(results_file)\n",
        "    analyze_results(final_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJbLqPXYOLvm",
        "outputId": "c8d30658-7594-4745-e4c2-3595c9a0acfb"
      },
      "outputs": [],
      "source": [
        "evaluate_cot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best-of-N \n",
        "\n",
        "The Best-of-N approach generates several candidate responses for a problem and then selects the one with the highest average token log-likelihood. This ensures that the final answer, formatted within the `\\boxed{}` command, is not only correct in presentation but also statistically the most reliable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1NylbVYkT1kx"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = '''You are solving mathematics problems.\n",
        "\n",
        "Please think step by step.\n",
        "\n",
        "Important: Always end your solution with the final answer in this format:\n",
        "\n",
        "\\\\[\n",
        "\\\\boxed{your_answer_here}\n",
        "\\\\]\n",
        "\n",
        "The entire answer should be contained completely within the \\\\boxed{} command.'''\n",
        "\n",
        "\n",
        "\n",
        "def best_of_n_response(problem, N=5):\n",
        "    best_answer = None\n",
        "    best_avg_likelihood = float('-inf')\n",
        "    best_responses = []\n",
        "    prompt = SYSTEM_PROMPT + \"\\n\" + problem\n",
        "\n",
        "    for t in range(N):\n",
        "\n",
        "        # TODO: Generate a response \n",
        "\n",
        "        \n",
        "        # TODO:  Iterate over each choice in the response and append lobprob of each tocken to token_logprobs (you can see a sample of response to see how to extract the token logprobs)\n",
        "        \n",
        "\n",
        "        # TODO: Calculate the average log-likelihood and store the response, answer(that is extracted with extract_answer()), and average log-likelihood \n",
        "        \n",
        "\n",
        "        \n",
        "    # TODO: Group the responses by the answer (multiple responses can have the same answer)\n",
        "\n",
        "    # TODO: Find the best answer based on the average likelihood\n",
        "    \n",
        "   \n",
        "\n",
        "    return best_answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate best of n\n",
        "\n",
        "* modify response generation part to evalute this method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jyCkE9ToOHNO"
      },
      "outputs": [],
      "source": [
        "def evaluate_best_of_n():\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    results_file = \"evaluation_results_math500_deepseek_best_of_n.json\"\n",
        "    dataset = load_math500_dataset()\n",
        "    existing_results = load_existing_results(results_file)\n",
        "    processed_indexes = {result['index'] for result in existing_results}\n",
        "    cnt = 0\n",
        "    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n",
        "        if idx in processed_indexes:\n",
        "            continue\n",
        "        if idx >= 30:\n",
        "          break\n",
        "        problem_text = item['problem']\n",
        "        correct_answer = extract_answer(item['solution'])\n",
        "        # TODO: ##########################################################\n",
        "        response = \n",
        "        predicted_answer = response\n",
        "        ##########################################################\n",
        "        is_correct = compare_answers(correct_answer, predicted_answer)\n",
        "        result = {\n",
        "            \"index\": idx,\n",
        "            \"problem\": problem_text,\n",
        "            \"response\": response,\n",
        "            \"correct_answer\": correct_answer,\n",
        "            \"predicted_answer\": predicted_answer,\n",
        "            \"is_correct\": is_correct\n",
        "        }\n",
        "        save_result(results_file, result)\n",
        "        if is_correct:\n",
        "          cnt += 1\n",
        "        print(f\"corrects :  {cnt} idx: {idx}\")\n",
        "    final_results = load_existing_results(results_file)\n",
        "    analyze_results(final_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "W_6D_Pb8V3tr",
        "outputId": "e87ea8e5-1b4e-40ce-d947-ec6142786be9"
      },
      "outputs": [],
      "source": [
        "evaluate_best_of_n()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Beam Search\n",
        "\n",
        "This cell implements a beam search strategy for generating candidate reasoning chains. The method generates multiple continuations at each reasoning step, scoring each candidate based on its average token log-likelihood. By retaining and expanding only the top candidates, the approach efficiently searches for the most promising chain-of-thought that leads to the final answer in the required format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "v2ZQr1A0WBD0"
      },
      "outputs": [],
      "source": [
        "def call_qwen_model_raw(prompt,step_num, temperature=0.8):\n",
        "    \"\"\"\n",
        "    Sends a request to the local Qwen endpoint and returns the generated text\n",
        "    along with the average token log-probability.\n",
        "    \"\"\"\n",
        "    # Build the prompt. We assume the sample already contains the SYSTEM_PROMPT. you can modify max_tokens for different steps\n",
        "    \n",
        "\n",
        "    # TODO: Send a request to the Qwen model and get the response \n",
        "    \n",
        "\n",
        "    # TODO:  Iterate over each choice in the response and append lobprob of each tocken to token_logprobs\n",
        "    token_logprobs =\n",
        "\n",
        "    # TODO: Calculate the average log-likelihood\n",
        "    \n",
        "    avg_token_prob = \n",
        "\n",
        "    return output_text, avg_token_prob, len(token_logprobs)\n",
        "\n",
        "\n",
        "\n",
        "class BeamCandidate:\n",
        "    def __init__(self, sequence, cumulative_log_prob, step_scores, finished=False,num_token = 0):\n",
        "        self.sequence = sequence\n",
        "        self.cumulative_log_prob = cumulative_log_prob\n",
        "        self.step_scores = step_scores\n",
        "        self.finished = finished\n",
        "        self.num_token = num_token\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"BeamCandidate(score={self.cumulative_log_prob:.3f}, finished={self.finished}, \"\n",
        "                f\"sequence={self.sequence})\")\n",
        "\n",
        "\n",
        "\n",
        "def generate_reasoning_steps(context, step_num, top_k):\n",
        "    \"\"\"\n",
        "    For a given candidate reasoning chain (context), generate top_k candidate continuations\n",
        "    for the current reasoning step (from 1 to 5). Each candidate is verified using the average\n",
        "    token logprob as a proxy for quality.\n",
        "    \"\"\"\n",
        "    \n",
        "    # TODO: each step should have a different prompt and the prompt should be added to the context so make a prompt for each step that explains what the step is about\n",
        "    candidates = []\n",
        "    for i in range(top_k):\n",
        "        if step_num == 1:\n",
        "            candidate_prompt = (\n",
        "                context +\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # TODO: call the qwen model to get the output and avg_token_prob\n",
        "        \n",
        "        \n",
        "       \n",
        "        candidates.append((candidate_step, avg_token_prob,num_token, finished))\n",
        "\n",
        "    return candidates\n",
        "\n",
        "def beam_search(init_problem_prompt, beam_width=3, max_steps=3, top_k=2):\n",
        "    \"\"\"\n",
        "    Implements a beam search over reasoning steps.\n",
        "    \"\"\"\n",
        "    prompt = \n",
        "    initial_candidate = \n",
        "    beams = \n",
        "\n",
        "\n",
        "    for step_num in range(1, max_steps+1):\n",
        "        new_beams = []\n",
        "\n",
        "        for candidate in beams:\n",
        "            if candidate.finished:\n",
        "                # TODO: Propagate finished candidates unchanged.\n",
        "                \n",
        "                continue\n",
        "            step_candidates = generate_reasoning_steps(candidate.sequence, step_num, top_k)\n",
        "            for (step_text, score,num_token, finished) in step_candidates:\n",
        "                # TODO: Create a new candidate by appending the step text to the current sequence and updating the log-probability by averaging all token_logprobs after the new step.\n",
        "\n",
        "\n",
        "        if not new_beams:\n",
        "            break\n",
        "        # TODO: sort the new beams based on the cumulative_log_prob and put the top beam_width beams in the beams list\n",
        "        beams =\n",
        "        \n",
        "\n",
        "        if all(beam.finished for beam in beams):\n",
        "            break\n",
        "    # TODO: Get the best candidate from the beams list that is finished\n",
        "    finished_beams = \n",
        "    \n",
        "    best_candidate = \n",
        "    return best_candidate\n",
        "\n",
        "def run_qwen_beam_search(problem,beam_width, max_steps, top_k, log_level):\n",
        "    \"\"\"\n",
        "    sets up the sample prompt, performs beam search,\n",
        "    and extracts the final answer.\n",
        "    \"\"\"\n",
        "    # TODO: Set the initial prompt to the problem and run the beam search to get the best candidate\n",
        "\n",
        "    \n",
        "\n",
        "  \n",
        "    return final_answer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate beam search\n",
        "* modify response generation part to evalute this method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "eR_TyvGMazTc"
      },
      "outputs": [],
      "source": [
        "def evaluate_beam_search():\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    results_file = \"evaluation_results_math500_deepseek_beam_search.json\"\n",
        "    dataset = load_math500_dataset()\n",
        "    existing_results = load_existing_results(results_file)\n",
        "    processed_indexes = {result['index'] for result in existing_results}\n",
        "    cnt = 0\n",
        "    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n",
        "        if idx in processed_indexes :\n",
        "            continue\n",
        "        if idx >= 30:\n",
        "          break\n",
        "        problem_text = item['problem']\n",
        "        correct_answer = extract_answer(item['solution'])\n",
        "        ##########################################################\n",
        "        # TODO: Generate a response with beam search\n",
        "        response = \n",
        "        predicted_answer = \n",
        "        ##########################################################\n",
        "        is_correct = compare_answers(correct_answer, predicted_answer)\n",
        "        result = {\n",
        "            \"index\": idx,\n",
        "            \"problem\": problem_text,\n",
        "            \"response\": response,\n",
        "            \"correct_answer\": correct_answer,\n",
        "            \"predicted_answer\": predicted_answer,\n",
        "            \"is_correct\": is_correct\n",
        "        }\n",
        "        save_result(results_file, result)\n",
        "        if is_correct:\n",
        "          cnt += 1\n",
        "        print(f\"corrects :  {cnt} idx: {idx}\")\n",
        "    final_results = load_existing_results(results_file)\n",
        "    analyze_results(final_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "sxMjVBf6bpiy",
        "outputId": "739ec144-b8e5-4403-feaf-f28e4b2f7eb9"
      },
      "outputs": [],
      "source": [
        "evaluate_beam_search()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Refinement \n",
        "\n",
        "This approach begins by generating an initial solution using the given prompt. It then iteratively refines this output by providing the model with targeted feedback and asking it to improve its response. The process continues until the feedback indicates that no further refinement is necessary, ensuring that the final answerâ€”properly formatted within the `\\boxed{}` commandâ€”is as accurate and well-reasoned as possible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fAYp7rCbiJQa"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = '''You are solving mathematics problems.\n",
        "\n",
        "Please think step by step.\n",
        "\n",
        "Important: Always end your solution with the final answer in this format:\n",
        "\n",
        "\\\\[\n",
        "\\\\boxed{your_answer_here}\n",
        "\\\\]\n",
        "\n",
        "The entire answer should be contained completely within the \\\\boxed{} command.'''\n",
        "\n",
        "\n",
        "\n",
        "def generate_content(prompt):\n",
        "\n",
        "    # TODO: Send a request to the Qwen model and get the response\n",
        "    \n",
        "\n",
        "    return output_text\n",
        "\n",
        "def self_refine(problem, max_iter=2):\n",
        "\n",
        "    prompt = SYSTEM_PROMPT + \"\\n\" + problem\n",
        "\n",
        "    # TODO: Generate the initial output using generate_content with the full prompt.\n",
        "    current_output = generate_content(prompt)\n",
        "\n",
        "    for iteration in range(max_iter):\n",
        "        # TODO: Provide a feedback prompt that asks the model to analyze current_output.\n",
        "        #       - Include both the original prompt and the current output\n",
        "        #       - speicfy the format if the feedback is needed so you can parse it later\n",
        "        \n",
        "        \n",
        "        # TODO: Send the feedback prompt to the model using generate_content and capture the feedback response.\n",
        "        \n",
        "        \n",
        "        # TODO: Parse the feedback response to determine if refinement is needed.\n",
        "        \n",
        "        \n",
        "        # TODO: If refinement is needed:\n",
        "        #       - Create a refine prompt that includes the original prompt, current output, and the feedback.\n",
        "        #       - Send this refine prompt to the model using generate_content to obtain a refined output.\n",
        "        #       - Update current_output with the refined output.\n",
        "        \n",
        "        \n",
        "    # TODO: Extract the final answer from current_output (e.g., using an extract_answer function).\n",
        "    return answer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate Self-Refinement\n",
        "* modify response generation part to evalute this method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ln5I9pg_jN7Z"
      },
      "outputs": [],
      "source": [
        "def evaluate_self_refiner():\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    results_file = \"evaluation_results_math500_deepseek_self_refiner.json\"\n",
        "    dataset = load_math500_dataset()\n",
        "    existing_results = load_existing_results(results_file)\n",
        "    processed_indexes = {result['index'] for result in existing_results}\n",
        "    cnt = 0\n",
        "    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n",
        "        if idx in processed_indexes :\n",
        "            continue\n",
        "        if idx >= 30:\n",
        "          break\n",
        "        problem_text = item['problem']\n",
        "        correct_answer = extract_answer(item['solution'])\n",
        "        ##########################################################\n",
        "        # TODO: Generate a response with self_refine\n",
        "        response = \n",
        "        predicted_answer = response\n",
        "        ##########################################################\n",
        "        is_correct = compare_answers(correct_answer, predicted_answer)\n",
        "        result = {\n",
        "            \"index\": idx,\n",
        "            \"problem\": problem_text,\n",
        "            \"response\": response,\n",
        "            \"correct_answer\": correct_answer,\n",
        "            \"predicted_answer\": predicted_answer,\n",
        "            \"is_correct\": is_correct\n",
        "        }\n",
        "        save_result(results_file, result)\n",
        "        if is_correct:\n",
        "          cnt += 1\n",
        "        print(f\"corrects :  {cnt} idx: {idx}\")\n",
        "    final_results = load_existing_results(results_file)\n",
        "    analyze_results(final_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "uNAg9wa9jlSu",
        "outputId": "64033f38-3340-4729-be0e-70dd0d792705"
      },
      "outputs": [],
      "source": [
        "evaluate_self_refiner()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
